<!-- Very simple website layout by me. Feel free to copy / adapt. -->

<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="Projects">

        <!-- To make this page not show up in search results -->
        <!-- <meta name="robots" content="noindex,nofollow"> -->

        <!-- More convenient links to assets in home folder -->
        <base href="../">

        <title>Raymond Liu | CS PhD student at Harvard</title>

        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js" defer></script>
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" defer></script>
        <link rel="stylesheet" href="assets_/style.css" media="screen">

        <!-- Font awesome 5 (for icons) -->
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">

        <link rel="icon" type="image/png" href="assets_/img/favicon_32x32.png" sizes="32x32" />
        <link rel="icon" type="image/png" href="assets_/img/favicon_16x16.png" sizes="16x16" />

    </head>

    <body>

    <header>
        <nav class="navbar navbar-expand-sm navbar-dark justify-content-center" style="background-color:#8c1515;">
            <a class="navbar-brand px-2" href="#">Raymond Liu</a>

            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="w-50">
                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="navbar-nav ml-auto">
                        <li class="px-1 nav-item"><a class="nav-link" href="">About</a></li>
                        <li class="px-1 nav-item active"><a class="nav-link" href="projects" onclick="event.preventDefault();">Projects</a></li>
                        <li class="px-1 nav-item"><a class="nav-link" href="https://rl27.github.io/cv">CV</a></li>
                        <li class="px-1 nav-item"><a class="nav-link" href="https://github.com/rl27" aria-label="GitHub">
                            <i class="fab fa-github" style="font-size:28px;" aria-hidden="true"></i>
                        </a></li>
                        <li class="px-1 nav-item"><a class="nav-link" href="https://www.linkedin.com/in/raymondzl" aria-label="LinkedIn">
                            <i class="fab fa-linkedin" style="font-size:28px;" aria-hidden="true"></i>
                        </a></li>
                    </ul>
                </div>
            </div>
        </nav>
    </header>

    <main class="container">
        <hr>

        <div class="row justify-content-center">
        
            <div class="col-md-11 text-center" id="proj-description">
                <p>This page contains some of my older projects from undergrad / high school.</p>
            </div>
        </div>

        <hr>
        <div class="col">
            <h4>Exploring Generative Models in Hyperbolic Space</h4>
        </div>
        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>Hyperbolic space is a generally under-explored topic, but it's particularly interesting because unlike Euclidean space, hyperbolic space expands exponentially due to having negative curvature, which makes it much more interesting to explore and far less limiting to explore.</p>
                <p>During summer 2021 I developed a system for visualizing and exploring generative models in simulated hyperbolic space. I implemented a model, projection, and a <a href="https://en.wikipedia.org/wiki/Order-5_square_tiling">square tiling system</a> for hyperbolic space in OpenGL and connected the model with a PGAN for generating correlated images based on geodesic distance.</p>
                <p>For my undergraduate senior thesis I extended this work by implementing a generalized tiling system and a text-to-image model (<a href="https://arxiv.org/abs/2111.13792">LAFITE</a>). I also ran simulated experiments to gauge the effectiveness of my system for finding imagined outputs from a model.</p>
                <p>A simpler <a href="https://play.unity.com/mg/other/build-rj5-1">demo version</a> of the project is available.</p>

                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <a class="btn btn-outline-primary" href="https://github.com/rl27/MercatorUnity">Code</a>
                        <a class="btn btn-outline-primary" href="assets_/pdf/exploring_generative_models.pdf">Report</a>
                    </div>
                </div>
            </div>

            <div class="col-md-4">
                <img class="img-fluid mb-2" src="assets_/img/unity1.png" alt="Generated outputs in hyperbolic space">
                <img class="img-fluid" src="assets_/img/unity2.png" alt="A (5,7) hyperbolic tiling">
            </div>
        </div>

        <hr>
        <div class="col">
            <h4>Convolutional Transformers for Inertial Navigation</h4>
        </div>
        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>For my junior independent work at Princeton, I implemented neural network architectures that improve on existing state-of-the-art architectures on the task of inertial navigation. This task uses measurements from inertial measurement units (IMUs), which contain an accelerometer and a gyroscope, to predict an objects position. IMUs are cheap, ubiquitous, energy-efficient, and used in a wide variety of applications.</p>
                <p>My models use a transformer encoder and incorporate convolutional layers to extract both global and local data relationships, and achieve better results than the previous best method.</p>

                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <a class="btn btn-outline-primary" href="https://github.com/rl27/ronin">Code</a>
                        <a class="btn btn-outline-primary" href="assets_/pdf/iw_report.pdf">Report</a>
                        <a class="btn btn-outline-primary" href="assets_/img/iw_poster.pdf">Poster</a>
                    </div>
                </div>
            </div>

            <div class="col-md-4">
                <img class="img-fluid" src="assets_/img/cvit.png" alt="Ground truth and predicted trajectories from various models.">
            </div>
        </div>

        <hr>
        <div class="col">
            <h4>OSCAR: Occluding Spatials, Category, And Region under discussion</h4>
        </div>
        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>For the final project for COS484 (Natural Language Processing) at Princeton, we reproduced a question-answering model and the ablations from <a href="https://aclanthology.org/2021.emnlp-main.390.pdf">this paper</a>. We additionally performed several of our own ablations, and tested using different models for generating question embeddings.</p>

                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <a class="btn btn-outline-primary" href="https://github.com/ind1010/Region-under-discussion-for-visual-dialog">Code</a>
                        <a class="btn btn-outline-primary" href="assets_/pdf/COS484_Report.pdf">Report</a>
                        <a class="btn btn-outline-primary" href="assets_/img/484poster.pdf">Poster</a>
                    </div>
                </div>
            </div>

            <div class="col-md-4">
                <img class="img-fluid" src="assets_/img/qcs_rud.png" alt="The QCS+RuD model.">
            </div>
        </div>

        <hr>
        <div class="col">
            <h4>Pedestrian Detection and Interpretability</h4>
        </div>
        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>For the final project for COS429 (Computer Vision) at Princeton, we investigated whether CNNs trained for object detections are reliant on visual cues when detecting pedestrians. We trained a Faster R-CNN on the Caltech Pedestrian Dataset, evaluated the model on different categories of pedestrian images, and improved the model by up-weighting images in poor-performing categories during training.</p>

                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <a class="btn btn-outline-primary" href="https://github.com/ind1010/pedestrian_detection_interpretability">Code</a>
                        <a class="btn btn-outline-primary" href="assets_/pdf/pedestrian_detection.pdf">Report</a>
                        <a class="btn btn-outline-primary" href="assets_/img/429poster.png">Poster</a>
                    </div>
                </div>
            </div>

            <div class="col-md-4">
                <img class="img-fluid" src="assets_/img/caltech_pedestrian.png" alt="Pedestrians">
                
            </div>
        </div>

        <!---
        <hr>
        <div class="col">
            <h4>Crowdsourcing Datasets for Optical Flow</h4>
        </div>
        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>During summer 2020, I joined the <a href="https://pvl.cs.princeton.edu/">Princeton Vision & Learning Lab</a> to work on a visual learning project on optical flow. I helped develop and optimize a system for collecting human-annotated images.</p>
                <p>These annotations are used to predict the ground truth optical flow for various scenes and videos.</p>
            </div>

            <div class="col-md-4">
                <img class="img-fluid mb-2" src="assets_/img/frame_0001.png" alt="Frame 1">
                <img class="img-fluid" src="assets_/img/frame_0008.png" alt="Frame 8">
                
            </div>
        </div>
        -->

        <hr>
        <div class="col">
            <h4>Ray Tracing</h4>
        </div>
        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>During a summer internship at Oregon State University I created a simple ray tracer using C++. I implemented and tested methods for improving image rendering speed and making rendered images more realistic.</p>
                <p>The top image is an output from an early version of the ray tracer that used simple methods such as antialiasing and reflection.</p>
                <p>The bottom image is an output that used more advanced techniques, such as Monte Carlo path tracing, to create more realistic lighting.</p>
                <p>I presented my work at the 2018 ASE Symposium <!--<a href="https://issuu.com/saturdayacademy/docs/2018symposium_r3">2018 ASE Symposium</a> (see p.25)--> at the University of Portland.</p>

                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <a class="btn btn-outline-primary" href="https://github.com/rl27/ray-tracing">Code</a>
                        <a class="btn btn-outline-primary" href="assets_/pdf/rt_poster.pdf">Poster</a>
                        <a class="btn btn-outline-primary" href="assets_/pdf/physically_based_rendering.pptx">Presentation</a>
                    </div>
                </div>
            </div>

            <div class="col-md-4">
                <img class="img-fluid mb-2" src="assets_/img/rt_spheres.png" alt="Spheres">
                <img class="img-fluid" src="assets_/img/path_tracing.png" alt="Path tracing">
            </div>
        </div>

        <!---
        <hr>
        <div class="col">
            <h4>Explainable Neural Networks</h4>
        </div>

        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>During summer of 2019 I worked on a project at Oregon State University on explainable neural networks. This project aimed to explain the decision-making of deep neural nets for image recognition in terms of human concepts.</p>
                <p>The program was trained to recognize and identify images of birds and then analyzed to see whether it focused on semantically meaningful concepts, such as "Eye" or "Crown" as in the image shown here.</p>
            </div>

            <div class="col-md-4">
                <img class="img-fluid" src="assets_/img/xnn.png" alt="XNN">
            </div>
        </div>
        -->

        <hr>
        <div class="col">
            <h4>Dementia Diagnosis</h4>
        </div>

        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>Over the course of 2016 and early 2017 I helped develop a project that used deep convolutional neural nets to analyze MRI scans and attempt to diagnose various stages of dementia. My motivation for this project comes from my family's history of Alzheimer's.</p>
                <p>By developing a technique that allowed the 3D MRI scans to each be split into hundreds of 2D "slices" as seen in the image, I was able to substantially improve the accuracy of the program.</p>
                <p>This project was submitted to the 2017 Central Western Oregon Science Expo and the subsequent Intel Northwest Science Expo, where it won several awards.</p>

                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <a class="btn btn-outline-primary" href="assets_/pdf/isef_poster.pdf">Poster</a>
                    </div>
                </div>
            </div>
                
            <div class="col-md-4">
                <img class="img-fluid" src="assets_/img/mri_scan_1.png" alt="2D MRI scan">
                <img class="img-fluid" src="assets_/img/mri_scan_2.png" alt="2D MRI scan">
                <img class="img-fluid" src="assets_/img/mri_scan_3.png" alt="2D MRI scan">
            </div>
        </div>
       
        <hr>
        <!-- <div class="col">
            <h4>Other Websites</h4>
        </div>

        <div class="row pl-3 pr-3 mb-3">
            <div class="col-md-8" id="description">
                <p>Aside from this website, I've developed several other websites and concepts for websites. One example is <a href="https://airinchina.github.io/">airinchina.github.io</a>, which I created as an informative supplement for a high school project on air pollution in China.</p>
                <p>A test website I created as a structure for my final project for Web Development (CS290) at OSU can be viewed <a href="assets_/riddet">here</a>. The full version had a functional database, but the search bar on the test version still works!</p>
            </div>
                
            <div class="col">
                <a href="https://airinchina.github.io/"><img class="img-fluid" src="assets_/img/airchina.png" alt="Air in China"></a>
            </div>
        </div>

        <div class="row pl-3 pr-3 mb-3">
            <div class="col-md-8" id="description">
                <p>The most recent and the largest-scale website I've worked on so far is TigerTools, which I worked on alongside two of my classmates and friends, Indu Panigrahi and Adam Rebei. The website connects a simple and intuitive interface with a variety of APIs, allowing Princeton students to quickly locate on-campus amenities, such as printers, scanners, water filling stations, caf√©s, and vending machines.</p>
                <p>The latest version of TigerTools is currently hosted on <a href="https://www.tigerapps.org/#direct">TigerApps</a>. It requires a Princeton account to access.</p>

                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <a class="btn btn-outline-primary" href="https://github.com/PrincetonUSG/TigerTools">Code</a>
                    </div>
                </div>
            </div>
                
            <div class="col-md-4">
                <img class="img-fluid mb-2" src="assets_/img/tigertools.png" alt="TigerTools landing page">
                <img class="img-fluid" src="assets_/img/tigertools2.png" alt="TigerTools interface">
            </div>
        </div>
        

        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>Another website I've developed is named QTPod, which provides a web interface for users to listen to podcasts with advertisements automatically blocked. The advertisements are detected with speech recognition and removed.</p>

                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <a class="btn btn-outline-primary" href="https://github.com/QTPod/qtpod-site">Code</a>
                    </div>
                </div>
            </div>
                
            <div class="col-md-4">
                <img class="img-fluid mb-2" src="assets_/img/qtpod_popular.png" alt="QTPod popular page">
                <img class="img-fluid" src="assets_/img/qtpod_single.png" alt="Podcast interface">
            </div>
        </div> -->

<!--         <hr>
        <div class="row">
            <img class="img-fluid mx-auto d-block" src="assets_/img/footer.png" alt="Footer" style="max-width:60%;">
        </div>
        <hr> -->

    </main>

    </body>

</html>
